{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Product reviews have become important part in decision making when considering online purchasing. People look at product descriptions and specifications to determine whether a product is a good fit to their requirement. However, in most cases, people have some specific question in their mind, e.g. “will this baby seat fit in the overhead compartment of 747?” which cannot be answered by product description as they contain generic information, and are not comprehensive i.e. missing crucial information that user needs. Consumer reviews, on the other hand, have a rich set of diversified information, based on each consumer’s individual experience. However, there can be many reviews associated with a product, which makes looking at every review time consuming\n",
    "\n",
    "Automatic answering of user query through plethora of consumer reviews is the hot topic of research in the Information Retrieval community. Can we design a system to output relevant reviews based on the user need?  We need a system to automatically answer product related binary (yes/No) queries from an existing dataset of consumer reviews. We plan to implement this by determining the relevance of a review with respect to a query [1] based on existing product reviews and then finding user sentiment for top relevant reviews to conclude the answer as yes or no.\n",
    "\n",
    "### For instance,\n",
    "\n",
    "**Product:** BRAVEN BRV-1 Wireless Bluetooth Speaker\n",
    "\n",
    "**Query:** \"I want to use this with my iPad air while taking a jacuzzi bath. Will the volume be loud enough over the bath jets?\"\n",
    "\n",
    "**Customer opinions ranked by relevance and vote:**\n",
    "\n",
    "* The sound quality is great, especially for the size, and if you place the speaker on a hard surface it acts as a sound board, and the bass really kicks up: **yes** \n",
    "* If you are looking for a water resistant blue tooth speaker you will be very pleased with this product: **yes** \n",
    "* However, if you are looking for something to throw a small party this just doesn’t have the sound output: **No**\n",
    "etc. etc.\n",
    "\n",
    "In the above scenario, we ranked the reviews based on their relevance score and used user sentiments to classify each review into yes/no. We use voting to conclude the answer as yes/no.\n",
    "\n",
    "\n",
    "# Problem statement\n",
    "\n",
    "Given a query $Q$ and set of reviews $R$ corresponding to a product $P$. We need to answer the query with the help of reviews as yes/no.\n",
    "\n",
    "# Related Work\n",
    "\n",
    "Our project aims to be at the interface of question-answering and opinion mining. Some previous work in relevance ranking includes word-level similarity approaches like Okapi BM25[2] and TF-IDF [3], grammatical rules and phrase-level approaches like ROGUE [4], and classical probabilistic language model [5]. They focused on learning the relevance or importance of some document, although the information learnt had not been used to answer queries. The work in the project is mainly focused on being as agnostic as possible in evaluating the relevance of an opinion, and thus a review in our case, to a query; and thus, training to learn this notion automatically from the review data accumulated. It is also differentiated from the existing Q/A techniques like Heterogeneous Model to decompose questions [6], and Identify expert and high quality answers [7,8,9] which are not centered around finding answers primarily, but rather to provide a relevance function that will help users navigate through a huge corpus of reviews and effectively find their answers based on most relevant subjective viewpoints and individual experiences.\n",
    "\n",
    "# Approach / Methods\n",
    "\n",
    "To address our goal, we’ll need a system with two components: (1) A relevance function, to determine which reviews contain information relevant to a query, and (2) a prediction function, allowing relevant reviews to 'vote' on the correct answer. Relevant reviews can act as expert to give opinion on the given query and we use prediction function to vote for yes/no.\n",
    "\n",
    "1. Relevance Function:\n",
    "    The reviews are characterized based on their expertise to provide a relevant answer to the corresponding binary question asked. The scoring function used to evaluate the relevance of a review can technically be any of the general similarity measures available to us; i.e. cosine similarity, ROGUE, BM25+ etc, or a mixture of these measures weighted through a voting scheme.\n",
    "    \n",
    "    The original paper mentions the relevance function as a parameterized scoring function:\n",
    "\n",
    "    $s(r,q) = \\phi (r,q).\\theta  + \\psi (q)M\\psi (r)^{T}$\n",
    "\n",
    "    Where, the first term $∅(r,q).θ$ is the pairwise similarity score taken by one of the general similarity measures, and $ψ(q)M〖ψ(r)〗^T$ is the bilinear scoring function.\n",
    "    \n",
    "    In our implementation we are using variance of state-of-the-art like TF-IDF method called BM25+, RougueL for finding logest common sequence and Bilinear model for considering synonyms.\n",
    "    \n",
    "2. Prediction function: \n",
    "    Given the relevance scores, we now need to have a voting scheme by which each review casts a Yes/No vote for the given question. Each question-review pair has a score associated with it, and these scores are weighted based on their relevance to the question. Each review gives a vote to the question, and the final score is a vote in favor of either a positive or negative answer. Here we are finding polarity of the review based on the setiments expressed in the review text. \n",
    "\n",
    "3. Mixture of experts\n",
    "    The mixture of experts model treats each review as an expert that votes either in favor or opposition of a response of Yes/No. It considers both the relevance of a review, or it’s confidence in its expertise, a review is considered to be an 'expert' in our case; and the vote that each such 'expert' casts.\n",
    "    The final classification can then be expressed as:\n",
    "$$P('Yes'|q) = \\sum_{r\\in R} P(r,q).P('Yes'|r,q)$$\n",
    "Where $P(r,q)$ = confidence in the ability of each review $r$ to classify question $q$ and $P(Yes|r,q)$ = the prediction of review $r$ to classify the response to question $q$ as $Yes$\n",
    "\n",
    "4. Non Binary answers: \n",
    "    User may need non-binary answers for a query. Finding relevant reviews that satisfy user need is important. LDA (Latent Dirichlet Allocation) is a way to automatically discover topics for any sentence. It can be used to divide question and all reviews into topics. Based on the question’s topic distribution, we can rank reviews where top reviews will have similar topic distribution as question.\n",
    "    LSA (Latent Semantic Analysis) can be used to group questions and reviews into concept(lower dimensional space).  Based on the question’s concept distribution, we can rank reviews where top reviews will have similar concept distribution as question.\n",
    "\n",
    "### Data set collections\n",
    "\n",
    "We used data from Amazon consisting of questions and reviews. Dataset has 1.4 million questions (and answers) on 191 thousand products, about which we have over 13 million customer reviews.\n",
    "\n",
    "Our formatted review dataset is in the following format:\n",
    "\n",
    "A3AEL89BFOJIU2 B005HKEJO6 4.0 1387670400 33 very nice and relatively easy to install . very fair price for what it is . led lighting is cool . a little loud . . . but that’s just being picky . Very nice and relatively easy to install. Very fair price for what it is. LED lighting is cool. A little loud...but that's just being picky.\n",
    "\n",
    "<img src=\"img_1.png\">\n",
    "\n",
    "It should be noted here that the ‘Review Summary’ contains the actual review twice, once in raw text, and again in the form formatted for our project – with casefolding implemented. The count of total no of words in the review gives us the count of words to be extracted.\n",
    "\n",
    "The formatted question-answer dataset is in the following format:\n",
    "Question:\n",
    "B005HKEJO6 Q YN 6011 6 does this vent to the outside does this vent to the outside\n",
    "\n",
    "<img src=\"img_2.png\">\n",
    "\n",
    "Answer:\n",
    "B005HKEJO6 A Y 6011 7 yes in my case through the ceiling Yes, in my case through the ceiling\n",
    "\n",
    "<img src=\"img_3.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Technical contributions\n",
    "\n",
    "1. We implemented relevance function to find out relevant reviews per question. These reviews will act as experts to vote for negative or positive answer. We used combination of BM25+, Rogue, Cosine and Bilinear to train the model. \n",
    "2.\tWe used voting function to find user sentiments, we classify the review into positive or negative sentiments and use top relevant reviews as experts to conclude the answer as yes or no.\n",
    "3.\tTo find top 5% of relevant reviews for non binary answer questions we are using LDA and LSI model which plots reviews and questions into same topics and concepts. \n",
    "\n",
    "### Challenges\n",
    "1. Implementing bilinear relevance function described in the paper was difficult due to lack of information on paper and scarcity of material available on internet.\n",
    "2. Finding out a different voting scheme to use relevant reviews as experts to answer the question as Yes/No.\n",
    "3. Finding the appropiate model to train reviews to answer open ended questions.\n",
    "4. Handling duplicates in questions and reviews dataset.\n",
    "5. Handling questions with no review datasets.\n",
    "\n",
    "We have used sentiments analysis to vote for positive or negative answer whereas paper uses bilinear model for same.\n",
    "\n",
    "# Evaluation.\n",
    "\n",
    "### Quantitative\n",
    "\n",
    "In our model we are dividing datasets into 90% training and 10% testing. In the training we are applying weight measure on pairwise similarity and bilinear model. \n",
    "\n",
    "1)      We use accuracy as a metric to evaluate our performance to predict yes/no answer to a query. We considered 5 datasets to train our model and tested it against ground truth.  Average accuracy indicates how good our system is in predicting the answer to binary query. We are getting good accuracy which shows that we are successful in predicting yes/no answer to a query.\n",
    "\n",
    " \n",
    "\n",
    "2)      We measured our performance with respect to measurements given in the paper. We can see both results are comparable.\n",
    "\n",
    " \n",
    "\n",
    "3)      We also compared our results with existing baseliners mentioned in the paper and found that our model is performing well as compared to a combination of weighted BM25+ and rogue implementation.\n",
    "\n",
    "\n",
    "### Qualitative (Shining part)\n",
    "\n",
    "To model non binary question answers LSI and LSA is used. We have used ad-hoc method to evaluate these two methods. We predict following question for product \"InSinkErator Badger 1, 1/3 HP Household Food Waste Disposer\" by both model using 200 reviews and returned top 10 most relevant reviews.\n",
    "\n",
    "From LSI:\n",
    "1. review 1 & 2 talks about the duration customer is using thias product, thus justifying question.\n",
    "2. review 3 is positive review but does not talk about durability issue.\n",
    "3. review 4 & 5 also talks about the durability.\n",
    "Thus, we can say LSI return majority of relevant reviews.\n",
    "\n",
    "From LDA:\n",
    "All are relevant as they all talk about duration this product lasted but indirectly.\n",
    "\n",
    "Thus, it can be said that both LDA and LSI model can be used to sort reviews according to the relevance with question.\n",
    "\n",
    "LSI: \"Is it long lasting?\"\n",
    "\n",
    "Answers:\n",
    "1. this unit came with my new constructed house . it finally went out after 12 . 5 years . i hope my new one last as long . \n",
    "\n",
    "2.  this model disposal was in my home when we first moved in and lasted us 9 years before it went on the blink and started to leak from the bottom . worked very well but a little noisy . replaced it with insinkerator badger 5 12 hp that has more power and is quieter hope its as reliable . \n",
    "\n",
    "3.  works great . great price \n",
    "\n",
    "4.  bought this disposal to replace an old badger 1 that id used for over 10 years . it came with the house so it could be much older . this one lasted less than a year and a half . i hope i just got a bad one but after looking at the other reviews i doubt it . im thinking their quality has gone downhill over the years at least with the badger line . just bought an evolution excel\n",
    "\n",
    "5.  this was an exact replacement for what i already had which was in the house when i purchased it over 12 years ago . i didnt have to change any of the drain mounting hardware . hopefully this one will last as long . \n",
    "\n",
    "LDA: \"Is it long lasting?\"\n",
    "\n",
    "Answer:\n",
    "1. after buying 3 of these in 15 years ive started to see a pattern . . my latest badger 1 started leaking through the various screwcircuit breaker holes in the bottom . on the previous ones owned i had a permanent leak on one and an unclearablejam on the other . all of these cheaper badger insinkerators have galvanized steel rotators and stainless blades . . . what that means is in about a month or two when you look into the unit it will seem horribly rusted because.........6 oz compared to 26 oz for the badger 1 more hp and a 4 year warranty . if i get 10 years out of this one it would be the same cost as two badger 1s lasting 5 years each . so you can decide for yourself . if you live in a softerwater area or dont demand much from your disposal or are a landlord this one may be for you . but based on my experience and apparently that of several other people in the reviews here you do seem to be giving up some longerterm reliability with the lessexpensive models . \n",
    "\n",
    "2.  i was cooking and grinding up some chicken bones when i slipped on the floor in a huge puddle of water . the water had filled my entire under sink area and flowed out onto the floor ruining my cabinet bases . after cleaning up the mess i took my bader apart for find the inside a rusted and crusty mess . the nut holding the grinding plate desinagrated when i tried to remove it with a socket wrench . . . . my thoughts of repairing the unit were a joke . the housing under the grinder plate was cracked and rusted beyond belief . a very sad pos is what the badger disposal is . . . . not a deal at any price . \n",
    "\n",
    "3.  worked well for about 6 years . then it silently damaged the underthesink cabinet before we realized it was leaking from the bottom screws . must be made with substandard materials . no where in the installation manual does it warn about this potential problem . company has no integrety . my suggestion to who ever built this thing would be to put in a more durable bottom . and this would not require educated engineers . just someone with common sense . \n",
    "\n",
    "4.  personally used this for about 3 12 years in my condo that was built in late 06 install date rusted through the bottom casing last night with two holes about the diameter of pencil lead and two more very soon to come . disposal was always noisy but seemed to have enough power for the basic tasks i would need it for . i was satisfied w the disposal but i would not buy again . \n",
    "\n",
    "5.  i thought the leaking was caused by worn out seals . no its caused by rusted out metal . the bottom of the end bell assembly which seperates the motor from the shredder is completely rusted . there is one hole already . i can poke through the iorn sheet at several other places . in other words the disposal can at most last about 4 years under light use . \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Conclusion \n",
    "\n",
    "The problem of question-answering, and more specifically using community sourced data to answer user questions has fascinated the researchers of Information Retrieval for a long time. Answering questions has advanced from evaluating simple probabilistic models such as Naïve Bayes on a bag-of-words model, to using advanced concepts from word embeddings to deep learning. We approached our problem statement with the intention of learning and implementing various information retrieval techniques, gaining exposure on how different relevance functions vary as per features of the dataset, the data size, nature etc. We implemented mixture-of-experts model on our binary question – answer dataset, and found that not only implementing a parameterized scoring function can be trained to learn the relevance of reviews, and subsequently cast vote with proportional confidence; but also saw that by using a bilinear model to transform two different word feature spaces to a common dimensional space improves our scoring function. We used LDA/LSA to divide the documents into lower dimensional space and learnt about dimensionality reduction. \n",
    "\n",
    "We took the help of various IR concepts that we learnt in class. We used weighted tf-idf scores to represent the question and review feature vectors. We used cosine similarity as a relevance score metric for binary question-answers; and used LSI and LDA to extract topic information from questions and reviews.\n",
    "\n",
    "Finally, in our evaluation we used accuracy metric to baseline and evaluate our classification model.\n",
    "\n",
    "There are some areas which can be used to extend the project. We are currently using bilinear transformation to depict similarity between the word features, we can use other techniques such as word2vec, word embeddings etc. These are known to be efficient for huge datasets, and for datasets which have a lot of associated sentences. These techniques can also be useful in dimensionality reduction, which will consequently be useful in improving both the time complexity and space complexity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Walkthrough\n",
    "\n",
    "\n",
    "This model is divided into two part, binary question and non binary question.\n",
    "\n",
    "### Binary questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import operator\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Corpus\n",
    "\n",
    "We extracted relevant information from the question and review dataset. Relevant information includes Product ID, question ID, review ID, cleaned questions and review text. Dataset have both open ended and binary question. Here we are considering only binary questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpus:\n",
    "    def __init__(self, quesfile, reviewfile):\n",
    "        self.question = dict()\n",
    "        self.review = dict()\n",
    "        self.data = dict()\n",
    "        self.count = 0\n",
    "        self.test_question = dict()\n",
    "        self.num_lines = 0\n",
    "        self.reviewfile = reviewfile\n",
    "        self.quesfile = quesfile\n",
    "\n",
    "    def create_dict(self):\n",
    "        num_lines = sum(1 for line in open(self.quesfile))\n",
    "        q = questions(self.quesfile, (0.90*self.num_lines)/2, self.data)\n",
    "        # taking care of questions from Amazon data\n",
    "        self.count, self.data, self.question, self.test_question= q.make_ques()\n",
    "        r = reviews(self.reviewfile, self.question, self.data, self.count)\n",
    "        # taking care of reviews from Amazon data\n",
    "        self.review, self.data = r.make_review()\n",
    "\n",
    "        unwanted = list()\n",
    "        for item in self.question:\n",
    "            if item not in self.review:\n",
    "                unwanted.append(item)\n",
    "\n",
    "        for item in unwanted:\n",
    "            self.question.pop(item, 0)\n",
    "            if item in self.test_question:\n",
    "                self.test_question.pop(item, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class questions:\n",
    "    def __init__(self, filename, ques_count, data):\n",
    "        self.ques = set()\n",
    "        self.tuning = 5000\n",
    "        self.question = dict()\n",
    "        self.test_question = dict()\n",
    "        self.file = filename\n",
    "        self.count = 0\n",
    "        self.ques_count = ques_count\n",
    "        self.data = data\n",
    "    \n",
    "        # Handling Question/answers here\n",
    "    def make_ques(self):\n",
    "        c = 0.0\n",
    "        # opening questions file from amazon data\n",
    "        with open(self.file) as fp:\n",
    "            for line in itertools.islice(fp, 1, self.tuning):\n",
    "                words = line.split()\n",
    "                if words[1] != \"A\":\n",
    "                    c += 1\n",
    "                # Not taking open ended question for now, only taking YN(binary) questions\n",
    "                if len(words) == 0 or words[2] == \"O\" or words[2] == \"?\":\n",
    "                    continue\n",
    "                if words[3] in self.ques:\n",
    "                    continue\n",
    "    \n",
    "                self.ques.add(words[3])\n",
    "    \n",
    "                # product id and looking for questions\n",
    "                if words[0] not in self.question and words[1] != \"A\":\n",
    "                    self.question[words[0]] = list()\n",
    "                if words[0] not in self.test_question and words[1] != \"A\" and c > self.ques_count:\n",
    "                    self.test_question[words[0]] = list()\n",
    "                # Handling anwers here\n",
    "                # looking for answer value, default answer is stored as False, making it True\n",
    "                if words[1] == \"A\" and words[2] == \"?\":\n",
    "                    if words[0] not in self.question:\n",
    "                        continue\n",
    "                    self.question[words[0]].pop()\n",
    "                    if c > self.ques_count and words[0] in self.test_question and len(self.test_question[words[0]]) !=0 :\n",
    "                        self.test_question[words[0]].pop()\n",
    "                    continue\n",
    "\n",
    "                if words[1] == \"A\" and words[2] == \"N\":\n",
    "                    # In case there is no question corresponsing to answer\n",
    "                    if words[0] not in self.question:\n",
    "                        continue\n",
    "                    # Storing answer as True for \"Y\" answers\n",
    "                    self.question[words[0]][-1][\"A\"] = False\n",
    "                    if c > self.ques_count and words[0] in self.test_question and len(self.test_question[words[0]]) !=0 :\n",
    "                        self.test_question[words[0]][-1][\"A\"] = False\n",
    "                    continue\n",
    "                # Handling questions here\n",
    "                if words[1] == \"Q\" and words[2] == \"YN\":\n",
    "    \n",
    "                    # for each question in an item\n",
    "                    freq = dict()\n",
    "                    q = list()\n",
    "                    # answer is False by default\n",
    "                    freq[\"A\"] = True\n",
    "                    # Storing all words correpsonding to questions in a dictionary\n",
    "                    freq[\"words\"] = dict()\n",
    "                    # To store sentence Id for Rogue implementation\n",
    "                    freq[\"sentence\"] = list()\n",
    "                    # Question ID\n",
    "                    freq[\"ID\"] = words[3]\n",
    "                    # traversing questions word by word\n",
    "                    for i in range(5, 5 + int(words[4])):\n",
    "                        if words[i] == \".\":\n",
    "                            continue\n",
    "                        # Each word is represented by an ID\n",
    "                        if words[i] not in self.data:\n",
    "                            self.data[words[i]] = self.count\n",
    "                            self.count += 1\n",
    "                        id = self.data[words[i]]\n",
    "                        # Increasing frequency of each word, initial 1 when first appear for first time in question\n",
    "                        if id not in freq[\"words\"]:\n",
    "                            freq[\"words\"][id] = 1\n",
    "                        else:\n",
    "                            freq[\"words\"][id] += 1\n",
    "                        # for Rogue\n",
    "                        freq[\"sentence\"].append(id)\n",
    "                    # appending freq for each question corresponding to item\n",
    "                    self.question[words[0]].append(freq)\n",
    "                    if c > self.ques_count:\n",
    "                        self.test_question[words[0]].append(freq)\n",
    "        \n",
    "        return self.count, self.data, self.question, self.test_question\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To populate dictionary\n",
    "import itertools\n",
    "\n",
    "class reviews:\n",
    "    def __init__(self, filename, question, data, count):\n",
    "        self.rev = set()\n",
    "        self.review = dict()\n",
    "        self.file = filename\n",
    "        self.count = count\n",
    "        self.learn = 50000\n",
    "        self.data = data\n",
    "        self.question = question\n",
    "    \n",
    "    # handling review part here\n",
    "    def make_review(self):\n",
    "        # opening Review data file from amazon data\n",
    "        with open(self.file) as fp:\n",
    "            for line in itertools.islice(fp, 1, self.learn):\n",
    "                words = line.split()\n",
    "                # if there is no questions for product, not considering reviews\n",
    "                if len(words) == 0 or words[1] not in self.question:\n",
    "                    continue\n",
    "                if words[0] in self.rev:\n",
    "                    continue\n",
    "    \n",
    "                self.rev.add(words[0])\n",
    "                # If product Id is not present in review\n",
    "                if words[1] not in self.review:\n",
    "                    self.review[words[1]] = list()\n",
    "                # considering each review\n",
    "                freq = dict()\n",
    "                # words in review\n",
    "                freq[\"words\"] = dict()\n",
    "                # Id correpsond to all words in sequence in a review for Rogue\n",
    "                freq[\"sentence\"] = list()\n",
    "                freq[\"actual_sen\"] = \"\"\n",
    "                # Review id\n",
    "                freq[\"ID\"] = words[0]\n",
    "                # traversing all the words in a review\n",
    "                for i in range(5, 5 + int(words[4])):\n",
    "                    if words[i] == \".\":\n",
    "                        continue\n",
    "                    # Assigning id to each word in whole review\n",
    "                    if words[i] not in self.data:\n",
    "                        self.data[words[i]] = self.count\n",
    "                        self.count += 1\n",
    "                    id = self.data[words[i]]\n",
    "                    # storing freq of each word\n",
    "                    if id not in freq[\"words\"]:\n",
    "                        freq[\"words\"][id] = 1\n",
    "                    else:\n",
    "                        freq[\"words\"][id] += 1\n",
    "                    # sentence will have id for each word\n",
    "                    freq[\"sentence\"].append(id)\n",
    "                    freq[\"actual_sen\"] += words[i] + \" \"\n",
    "                # appending to the list of review per product\n",
    "                self.review[words[1]].append(freq)\n",
    "                \n",
    "        return self.review, self.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Relevant reviews\n",
    "\n",
    "Here we are finding revelance between query and review. We want our scoring function to be parameterized so that we can learn from training data what constitutes a 'relevant' review. Thus we define a parameterized scoring functin by calculating following:\n",
    "\n",
    "Cosine is used as feature for bilinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class similarity_fact:\n",
    "    \n",
    "    def __init__(self, review, question):\n",
    "        self.review_data = review\n",
    "        self.question_data = question\n",
    "\n",
    "    def evaluate_cosine_similarity(self):\n",
    "        tf_scores_questions = self.evaluate_tf_questions(self.question_data)\n",
    "        tf_scores_reviews = self.evaluate_tf_reviews(self.question_data, self.review_data)\n",
    "        idf_scores = self.evaluate_idf(self.question_data, self.review_data)\n",
    "        tf_idf_questions = self.evaluate_tf_idf_qstns(tf_scores_questions, idf_scores)\n",
    "        tf_idf_reviews = self.evaluate_tf_idf_reviews(tf_scores_questions, tf_scores_reviews, idf_scores)\n",
    "        cosine = self.evaluate_cosine(tf_idf_questions, tf_idf_reviews)\n",
    "        return cosine,tf_idf_questions,tf_idf_reviews\n",
    "    \n",
    "    def evaluate_cosine(self, tf_idf_questions, tf_idf_reviews):\n",
    "        cosine_score = defaultdict()\n",
    "        for prod_key in tf_idf_questions:\n",
    "            if prod_key in tf_idf_reviews:\n",
    "                cosine_score[prod_key] = defaultdict(lambda: defaultdict())\n",
    "                for question_id,each_question in tf_idf_questions[prod_key].iteritems():\n",
    "                    question_length = 0\n",
    "                    temp_tf_idf_multiplied = defaultdict(lambda: 0)\n",
    "                    for key1, question_tf_idf in each_question.iteritems():\n",
    "                        question_length += math.pow(question_tf_idf,2)\n",
    "                    question_length = math.sqrt(question_length)\n",
    "                    for review_id,each_review in tf_idf_reviews[prod_key].iteritems():\n",
    "                        review_length = 0\n",
    "                        score = 0\n",
    "                        for key2,question_tf_idf in each_question.iteritems():\n",
    "                            if key2 not in each_review:\n",
    "                                review_tf_idf = 0\n",
    "                            else:\n",
    "                                review_tf_idf = each_review[key2]\n",
    "                            score += question_tf_idf*review_tf_idf\n",
    "                        for key3, review_tf_idf in each_review.iteritems():\n",
    "                            review_length += math.pow(review_tf_idf,2)\n",
    "                        review_length = math.sqrt(review_length)\n",
    "                        if review_length == 0 or question_length == 0:\n",
    "                            temp_tf_idf_multiplied[review_id] = 0\n",
    "                        else:\n",
    "                            temp_tf_idf_multiplied[review_id] = score/(review_length*question_length)\n",
    "                    cosine_score[prod_key][question_id] = temp_tf_idf_multiplied\n",
    "        return cosine_score\n",
    "    \n",
    "    \n",
    "    def evaluate_tf_idf_reviews(self, tf_scores_questions, tf_scores_reviews, idf_scores):\n",
    "        tf_idf_score = defaultdict()\n",
    "        for prod_key in tf_scores_questions:\n",
    "            if prod_key in tf_scores_reviews:\n",
    "                tf_idf_score[prod_key] = defaultdict()\n",
    "                review_id = -1\n",
    "                for r_id,each_review in tf_scores_reviews[prod_key].iteritems():\n",
    "                    review_id += 1\n",
    "                    scores = defaultdict(lambda: 0)\n",
    "                    for key, value in each_review.iteritems():\n",
    "                        if prod_key in idf_scores:\n",
    "                            idf_value = idf_scores[prod_key][key]\n",
    "                        else:\n",
    "                            idf_value = 0\n",
    "                            idf_scores[prod_key][key] = idf_value\n",
    "                        tf_idf = value * idf_value\n",
    "                        scores[key] = tf_idf\n",
    "                    # tf_idf_score[prod_key].append(scores)\n",
    "                    tf_idf_score[prod_key][r_id] = scores\n",
    "        return tf_idf_score\n",
    "    \n",
    "    \n",
    "    def evaluate_tf_idf_qstns(self, tf_scores_questions, idf_scores):\n",
    "        tf_idf_score = defaultdict()\n",
    "        for prod_key in tf_scores_questions:\n",
    "            if prod_key not in tf_idf_score:\n",
    "                tf_idf_score[prod_key] = defaultdict()\n",
    "            question_id = -1\n",
    "            for q_id,each_question in tf_scores_questions[prod_key].iteritems():\n",
    "                question_id += 1\n",
    "                scores = defaultdict(lambda: 0)\n",
    "                for key, value in each_question.iteritems():\n",
    "                    if prod_key in idf_scores and key in idf_scores[prod_key]:\n",
    "                        idf_value = idf_scores[prod_key][key]\n",
    "                    else:\n",
    "                        idf_value = 0\n",
    "                        # idf_scores[prod_key][key] = idf_value\n",
    "                    tf_idf = value * idf_value\n",
    "                    scores[key] = tf_idf\n",
    "                tf_idf_score[prod_key][q_id] = scores\n",
    "                # tf_idf_score[prod_key].append(scores)\n",
    "        return tf_idf_score\n",
    "    \n",
    "    \n",
    "    def evaluate_tf_reviews(self,question_data, review_data):\n",
    "        tf_scores = defaultdict()\n",
    "        for prod_key, question in question_data.iteritems():\n",
    "            if prod_key in review_data:\n",
    "                if prod_key not in tf_scores:\n",
    "                    tf_scores[prod_key] = defaultdict()\n",
    "                for review in review_data[prod_key]:\n",
    "                    term_count = defaultdict(lambda: 0)\n",
    "                    words = review['words']\n",
    "                    r_id = review['ID']\n",
    "                    for key, count in words.iteritems():\n",
    "                        term_count[key] = 1 + math.log(float(count))\n",
    "                    tf_scores[prod_key][r_id] = term_count\n",
    "                    # tf_scores[prod_key] = term_count\n",
    "        return tf_scores\n",
    "    \n",
    "    \n",
    "    def evaluate_tf_questions(self,question_data):\n",
    "        tf_scores = defaultdict()\n",
    "        for prod_key, question in question_data.iteritems():\n",
    "            if prod_key not in tf_scores:\n",
    "                tf_scores[prod_key] = defaultdict()\n",
    "            for each_question in question:\n",
    "                term_count = defaultdict(lambda: 0)\n",
    "                words = each_question['words']\n",
    "                q_id = each_question['ID']\n",
    "                for key, count in words.iteritems():\n",
    "                    term_count[key] = 1 + math.log(float(count))\n",
    "                tf_scores[prod_key][q_id] = term_count\n",
    "        return tf_scores\n",
    "    \n",
    "    \n",
    "    def evaluate_idf(self,question_data, review_data):\n",
    "        collection_freq = defaultdict()\n",
    "        for prod_key, question in question_data.iteritems():\n",
    "            term_count = defaultdict(lambda: 0)\n",
    "            review_count = 0\n",
    "            if prod_key in review_data:\n",
    "                for review in review_data[prod_key]:\n",
    "                    review_count += 1\n",
    "                    words = review['words']\n",
    "                    for key, count in words.iteritems():\n",
    "                        term_count[key] += 1\n",
    "                term_count['review_count'] = review_count\n",
    "                collection_freq[prod_key] = term_count\n",
    "    \n",
    "        idf_scores = defaultdict()\n",
    "        for prod in collection_freq:\n",
    "            idf = defaultdict(lambda: 0)\n",
    "            term_count = collection_freq[prod]\n",
    "            for key, count in term_count.iteritems():\n",
    "                idf_value = math.log(float(term_count['review_count']) / float(count))\n",
    "                idf[key] = idf_value\n",
    "            idf_scores[prod] = idf\n",
    "        return idf_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Bilinear Model: To give same weightage to two words who are synonym of each other, we make use of biliear transformation model.\n",
    "    It makes use of matrix where each cell[i][j] represents relationship between ith and jth word of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_bilinear(cosine, tf_idf_questions, tf_idf_reviews):\n",
    "    bilinear_score = defaultdict(lambda: defaultdict())\n",
    "    for prod_key in cosine:\n",
    "        bilinear_score_per_product = evaluate_bilinear_per_product(cosine[prod_key], tf_idf_questions[prod_key], tf_idf_reviews[prod_key])\n",
    "        bilinear_score[prod_key] = bilinear_score_per_product\n",
    "    return bilinear_score\n",
    "\n",
    "def evaluate_bilinear_per_product(cosine_product, tf_idf_questions_product, tf_idf_reviews_product):\n",
    "    weights_questions, weights_reviews, d, dx, dy = preprocessing_per_product(cosine_product, \\\n",
    "                                        tf_idf_questions_product, tf_idf_reviews_product)\n",
    "    beta = gamma = 0.01\n",
    "    theta_x = theta_y = 1\n",
    "    L_x, L_y = RMLS_per_product(weights_questions,weights_reviews,d,beta,gamma,theta_x, theta_y)\n",
    "    bilinear_per_prod = calculate_score(tf_idf_questions_product,L_x,L_y,tf_idf_reviews_product,d,dx,dy)\n",
    "    return bilinear_per_prod\n",
    "\n",
    "def calculate_score(tf_idf_questions,L_x, L_y,tf_idf_reviews,d,dx,dy):\n",
    "    final_score_list = defaultdict(lambda :defaultdict())\n",
    "    for each_question in tf_idf_questions:\n",
    "        score_sum = 0\n",
    "        prod1 = [0] * d\n",
    "        for each_question_word in tf_idf_questions[each_question].keys():\n",
    "            for i in range(0,d):\n",
    "                prod1[i]+= tf_idf_questions[each_question][each_question_word]*L_x[each_question_word][i]\n",
    "        prod2 = defaultdict(lambda :0)\n",
    "        for each_review_word in L_y.keys():\n",
    "            for j in range(0,d):\n",
    "                prod2[each_review_word] += L_y[each_review_word][j]*prod1[j]\n",
    "        score_list = defaultdict(lambda :0)\n",
    "        for each_review in tf_idf_reviews:\n",
    "            score = 0\n",
    "            for each_review_word in tf_idf_reviews[each_review]:\n",
    "                score += prod2[each_review_word]*tf_idf_reviews[each_review][each_review_word]\n",
    "            score_list[each_review] = score\n",
    "            score_sum += math.pow(score,2)\n",
    "        for each_score in score_list:\n",
    "            if score_sum != 0:\n",
    "                score_list[each_score] = score_list[each_score]/math.sqrt(score_sum)\n",
    "        final_score_list[each_question]=score_list\n",
    "        return final_score_list\n",
    "\n",
    "\n",
    "\n",
    "def RMLS_per_product(weights_questions, weights_reviews, d, beta, gamma, theta_x, theta_y):\n",
    "    # initialize L_x\n",
    "    L_x = defaultdict()\n",
    "    L_y = defaultdict()\n",
    "    for question_word in weights_questions:\n",
    "        l_xu = [random.random() for _ in range(d)]\n",
    "        L_x[question_word] = l_xu\n",
    "    for review_word in weights_reviews:\n",
    "        l_yv = [random.random() for _ in range(d)]\n",
    "        L_y[review_word] = l_yv\n",
    "\n",
    "    T = 10 #setting the convergence limit\n",
    "    t = 0\n",
    "    while t<=T:\n",
    "        t += 1\n",
    "        for question_word in weights_questions:\n",
    "            omega_u = [0] * d\n",
    "            for review_word in weights_reviews:\n",
    "                col = L_y[review_word]\n",
    "                for i in range(0,len(col)):\n",
    "                    omega_u[i] += col[i]*weights_questions[question_word][review_word]\n",
    "            l_xu = L_x[question_word]\n",
    "            non_zero = False\n",
    "            for i in range(0,len(l_xu)):\n",
    "                if l_xu[i] != 0:\n",
    "                    non_zero = True\n",
    "                    break\n",
    "            if non_zero == False:\n",
    "                l_xu = [0] * d\n",
    "            else:\n",
    "                length = 0\n",
    "                for z in range (0,d):\n",
    "                    positive = False\n",
    "                    if omega_u[z] > 0:\n",
    "                        positive = True\n",
    "                    z_th_element = abs(omega_u[z])\n",
    "                    max_value = max(z_th_element-beta,0)\n",
    "                    if positive:\n",
    "                        pass\n",
    "                    else:\n",
    "                        max_value = -max_value\n",
    "                    l_xu[z] = max_value\n",
    "                    length += math.pow(max_value,2)\n",
    "                length = math.sqrt(length)\n",
    "                for z in range(0,d):\n",
    "                    if length == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        l_xu[z] = l_xu[z] * (theta_x/length)\n",
    "\n",
    "            L_x[question_word] = l_xu\n",
    "\n",
    "        for review_word in weights_reviews:\n",
    "            eta_v = [0] * d\n",
    "            for question_word in weights_questions:\n",
    "                col = L_x[question_word]\n",
    "                for i in range(0, len(col)):\n",
    "                    eta_v[i] += col[i] * weights_reviews[review_word][question_word]\n",
    "            l_yv = L_y[review_word]\n",
    "            non_zero = False\n",
    "            for i in range(0, len(l_yv)):\n",
    "                if l_yv[i] != 0:\n",
    "                    non_zero = True\n",
    "                    break\n",
    "            if non_zero == False:\n",
    "                l_yv = [0] * d\n",
    "            else:\n",
    "                length = 0\n",
    "                for z in range(0, d):\n",
    "                    positive = False\n",
    "                    if eta_v[z] > 0:\n",
    "                        positive = True\n",
    "                    z_th_element = abs(eta_v[z])\n",
    "                    max_value = max(z_th_element - gamma, 0)\n",
    "                    if positive:\n",
    "                        pass\n",
    "                    else:\n",
    "                        max_value = -max_value\n",
    "                    l_yv[z] = max_value\n",
    "                    length += math.pow(max_value, 2)\n",
    "                length = math.sqrt(length)\n",
    "                for z in range(0, d):\n",
    "                    if length == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        l_yv[z] = l_yv[z] * (theta_y / length)\n",
    "\n",
    "            L_y[review_word] = l_yv\n",
    "\n",
    "    return L_x, L_y\n",
    "\n",
    "\n",
    "def preprocessing_per_product(cosine_product, tf_idf_questions_product, tf_idf_reviews_product):\n",
    "    question_vocab = set()\n",
    "    review_vocab = set()\n",
    "    # calculate dx and dy (size of vocab for all questions, all reviews respectively)\n",
    "    for question_id in tf_idf_questions_product:\n",
    "        question_vocab = question_vocab.union(tf_idf_questions_product[question_id].keys())\n",
    "    for review_id in tf_idf_reviews_product:\n",
    "        review_vocab = review_vocab.union(tf_idf_reviews_product[review_id].keys())\n",
    "    dx = len(question_vocab)  # size of question vocab\n",
    "    dy = len(review_vocab)  # size of review vocab\n",
    "    question_vocab_list = list(question_vocab)\n",
    "    review_vocab_list = list(review_vocab)\n",
    "    weights_questions = defaultdict(lambda: defaultdict)\n",
    "    weights_reviews = defaultdict(lambda: defaultdict)\n",
    "    # Initializing everything to 0\n",
    "    for u in range(0, dx):\n",
    "        weights_xu = defaultdict(lambda: 0)\n",
    "        for v in range(0, dy):\n",
    "            weights_xu[review_vocab_list[v]] = 0\n",
    "        weights_questions[question_vocab_list[u]] = weights_xu\n",
    "    for v in range(0, dy):\n",
    "        weights_yv = defaultdict(lambda: 0)\n",
    "        for u in range(0, dx):\n",
    "            weights_yv[question_vocab_list[u]] = 0\n",
    "        weights_reviews[review_vocab_list[v]] = weights_yv\n",
    "    # Precalculation\n",
    "    nx = len(tf_idf_questions_product)  # num questions\n",
    "    ny = len(tf_idf_reviews_product)  # num reviews\n",
    "    for key in weights_questions:\n",
    "        for question_id in cosine_product:\n",
    "            for review_id in cosine_product[question_id]:\n",
    "                for review_word in review_vocab_list:\n",
    "                    num = float(1) / float(nx * ny)\n",
    "                    try:\n",
    "                        num *= tf_idf_questions_product[question_id][key]\n",
    "                    except KeyError:\n",
    "                        num = 0\n",
    "                    num *= cosine_product[question_id][review_id]\n",
    "                    try:\n",
    "                        num *= tf_idf_reviews_product[review_id][review_word]\n",
    "                    except KeyError:\n",
    "                        num = 0\n",
    "                    weights_questions[key][review_word] += num\n",
    "\n",
    "    for key in weights_reviews:\n",
    "        for question_id in cosine_product:\n",
    "            for review_id in cosine_product[question_id]:\n",
    "                for question_word in question_vocab_list:\n",
    "                    num = float(1) / float(nx * ny)\n",
    "                    try:\n",
    "                        num *= tf_idf_reviews_product[review_id][key]\n",
    "                    except KeyError:\n",
    "                        num = 0\n",
    "                    num *= cosine_product[question_id][review_id]\n",
    "                    try:\n",
    "                        num *= tf_idf_questions_product[question_id][question_word]\n",
    "                    except KeyError:\n",
    "                        num = 0\n",
    "                    weights_reviews[key][question_word] += num\n",
    "\n",
    "    d = (min(dx,dy)/3) + 1\n",
    "\n",
    "    return weights_questions, weights_reviews, d, dx, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There can be common sequences between review and query which can be used to calculate their relavance score.\n",
    "Rogue-L calculates the length of longest common subsequence in the query and review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class roguel:\n",
    "\n",
    "    def __init__(self, review, question):\n",
    "        self.pairscore = dict()\n",
    "        self.review = review\n",
    "        self.question = question\n",
    "\n",
    "    # Dynamic Programming implementation of LCS problem\n",
    "    def lcs(self, X, Y):\n",
    "        # find the length of the strings\n",
    "        m = len(X)\n",
    "        n = len(Y)\n",
    "    \n",
    "        # declaring the array for storing the dp values\n",
    "        L = [[None] * (n + 1) for i in xrange(m + 1)]\n",
    "    \n",
    "        \"\"\"Following steps build L[m+1][n+1] in bottom up fashion\n",
    "        Note: L[i][j] contains length of LCS of X[0..i-1]\n",
    "        and Y[0..j-1]\"\"\"\n",
    "        for i in range(m + 1):\n",
    "            for j in range(n + 1):\n",
    "                if i == 0 or j == 0:\n",
    "                    L[i][j] = 0\n",
    "                elif X[i - 1] == Y[j - 1]:\n",
    "                    L[i][j] = L[i - 1][j - 1] + 1\n",
    "                else:\n",
    "                    L[i][j] = max(L[i - 1][j], L[i][j - 1])\n",
    "    \n",
    "        # L[m][n] contains the length of LCS of X[0..n-1] & Y[0..m-1]\n",
    "        return L[m][n]\n",
    "    # end of function lcs\n",
    "\n",
    "\n",
    "\n",
    "    def rogueL(self):\n",
    "        # r = dict()\n",
    "        for item in self.question:\n",
    "            # r[item] = list()\n",
    "            num = 0.0\n",
    "            for q in self.question[item]:\n",
    "                # res = list()\n",
    "                if item not in self.review:\n",
    "                    continue\n",
    "                if item not in self.pairscore:\n",
    "                    self.pairscore[item] = dict()\n",
    "                if q[\"ID\"] not in self.pairscore[item]:\n",
    "                    self.pairscore[item][q[\"ID\"]] = dict()\n",
    "                for val in self.review[item]:\n",
    "                    if val[\"ID\"] not in self.pairscore[item][q[\"ID\"]]:\n",
    "                        self.pairscore[item][q[\"ID\"]][val[\"ID\"]] = dict()\n",
    "                    self.pairscore[item][q[\"ID\"]][val[\"ID\"]][\"ROGUE\"] = self.lcs(q[\"sentence\"], val[\"sentence\"])\n",
    "                    num += self.pairscore[item][q[\"ID\"]][val[\"ID\"]][\"ROGUE\"]\n",
    "                for val in self.review[item]:\n",
    "                    if num != 0:\n",
    "                        self.pairscore[item][q[\"ID\"]][val[\"ID\"]][\"ROGUE_NORM\"] = self.pairscore[item][q[\"ID\"]][val[\"ID\"]][\"ROGUE\"]/num\n",
    "                    else:\n",
    "                        self.pairscore[item][q[\"ID\"]][val[\"ID\"]][\"ROGUE_NORM\"] = 0\n",
    "                    #print \"one review done\"\n",
    "                    # r[item].append(res)\n",
    "        \n",
    "        return self.pairscore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Frequency of words and document frequency plays an important role to calculate relevance score. BM25+ is \"TF-IDF\" model which make use of words frequencies in the query and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class bm25:\n",
    "    def __init__(self, review, question, pairscore):\n",
    "        self.review = review\n",
    "        self.question = question\n",
    "        self.pairscore = pairscore\n",
    "        \n",
    "    def getIDF(self, ques, review_l):\n",
    "        count = 0\n",
    "        for r in review_l:\n",
    "            if ques in r[\"words\"]:\n",
    "                count += 1\n",
    "    \n",
    "        num = len(review_l)-count+0.5\n",
    "        den = count+0.5\n",
    "        idf_val = math.log(num) - math.log(den)\n",
    "        return idf_val\n",
    "\n",
    "    def get_avgdl(self, review_l):\n",
    "        length = 0\n",
    "        for r in review_l:\n",
    "            length += len(r[\"sentence\"])\n",
    "        avg = length/len(review_l)\n",
    "        #nneed to fix this\n",
    "        if avg == 0:\n",
    "            avg == 1\n",
    "        return avg\n",
    "\n",
    "    def okapi(self):\n",
    "        k1, b = (1.2+2)/2, 0.75\n",
    "    \n",
    "        for item in self.question:\n",
    "            if item not in self.review:\n",
    "                continue\n",
    "            if item not in self.pairscore:\n",
    "                self.pairscore[item] = dict()\n",
    "            for q in self.question[item]:\n",
    "                if q[\"ID\"] not in self.pairscore[item]:\n",
    "                    self.pairscore[item][q[\"ID\"]] = dict()\n",
    "                # calculate avgdl\n",
    "                avgdl = self.get_avgdl(self.review[item])\n",
    "                norm_bm, norm_bmm = 0,0\n",
    "                #find term frequency\n",
    "                for r in self.review[item]:\n",
    "                    if r[\"ID\"] not in self.pairscore[item][q[\"ID\"]]:\n",
    "                        self.pairscore[item][q[\"ID\"]][r[\"ID\"]] = dict()\n",
    "                    score = 0\n",
    "                    index = 0\n",
    "                    score1 = 0\n",
    "                    for word in q[\"words\"]:\n",
    "                        # taking review as doc get idf for each word in query\n",
    "                        idf = self.getIDF(word, self.review[item])\n",
    "                        if word not in r[\"words\"]:\n",
    "                            w = 0\n",
    "                        else:\n",
    "                            w = r[\"words\"][word]\n",
    "                        num = (k1 +1)*w\n",
    "                        den = w + k1 *(1-b +b *(len(r[\"sentence\"])/avgdl))\n",
    "                        score += idf*(num/den)\n",
    "                        score1+=idf\n",
    "                    self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25\"] = score\n",
    "                    self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25+\"] = score1\n",
    "                    norm_bm += score\n",
    "                    norm_bmm += score1\n",
    "                for r in self.review[item]:\n",
    "                    if norm_bm != 0:\n",
    "                        self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25\"] /= norm_bm\n",
    "                    if norm_bmm !=0:\n",
    "                        self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25+\"] /= norm_bmm\n",
    "                    self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25+\"] += self.pairscore[item][q[\"ID\"]][r[\"ID\"]][\"BM25\"]\n",
    "                    self.pairscore[item][q[\"ID\"]][r[\"ID\"]].pop(\"BM25\", 0)\n",
    "                    #print q[\"ID\"], r[\"ID\"]\n",
    "        return self.pairscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Weight training is done so as to determine how they should be combined in order to achieve the best ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(pairscore,bilinear_score,question):\n",
    "    eta = 0.01\n",
    "    T = 500\n",
    "    t = 0\n",
    "    final_weight = defaultdict(lambda: defaultdict())\n",
    "    for prod_key in bilinear_score:\n",
    "        for each_question in bilinear_score[prod_key]:\n",
    "            question_weight = defaultdict(lambda: list())\n",
    "            w = [random.random() for _ in range(3)]\n",
    "            sum_weights = 0\n",
    "            for i in range(0,3):\n",
    "                sum_weights+=w[i]\n",
    "            if (sum_weights != 0.0):\n",
    "                for i in range(0,3):\n",
    "                    w[i]=w[i]/sum_weights\n",
    "            for each_review in bilinear_score[prod_key][each_question]:\n",
    "                t = 0\n",
    "                while t < T:\n",
    "                    t += 1\n",
    "                    sum = pairscore[prod_key][each_question][each_review]['ROGUE']+\\\n",
    "                        pairscore[prod_key][each_question][each_review]['BM25+']+\\\n",
    "                        bilinear_score[prod_key][each_question][each_review]\n",
    "                    scores = [pairscore[prod_key][each_question][each_review]['ROGUE']/sum, \\\n",
    "                                pairscore[prod_key][each_question][each_review]['BM25+']/sum, \\\n",
    "                                bilinear_score[prod_key][each_question][each_review]/sum]\n",
    "                    total_sum = w[0] * pairscore[prod_key][each_question][each_review]['ROGUE'] + \\\n",
    "                        w[1] * pairscore[prod_key][each_question][each_review]['BM25+'] + \\\n",
    "                        w[2] * bilinear_score[prod_key][each_question][each_review]\n",
    "                    if total_sum > 0.5:\n",
    "                        if question[prod_key][0]['A'] == False:\n",
    "                            w[0] -= eta\n",
    "                            w[1] -= eta\n",
    "                            w[2] -= eta\n",
    "                    else:\n",
    "                        if question[prod_key][0]['A'] == True:\n",
    "                            w[0] += eta\n",
    "                            w[1] += eta\n",
    "                            w[2] += eta\n",
    "                    sum_weights = 0\n",
    "                    for i in range(0,3):\n",
    "                        sum_weights+=w[i]\n",
    "                    if (sum_weights != 0.0):\n",
    "                        for i in range(0,3):\n",
    "                            w[i]=w[i]/sum_weights\n",
    "                question_weight[each_question] = w\n",
    "        final_weight[prod_key] = question_weight\n",
    "    return final_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting score\n",
    "\n",
    "Once the review is ranked in the order of their relevant score, top 5 reviews are taken and sentiment analysis is done on the text to categorize as positive or negative response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classification(predicted_model, test_pairscore, test_bilinear_score, test_question, question, review):\n",
    "    total_no = 0\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    c , i = 0, 0\n",
    "    relevant_reviews = defaultdict(lambda: defaultdict())\n",
    "    for prod_key in test_bilinear_score:\n",
    "        question_review_score = defaultdict(lambda: defaultdict())\n",
    "        for each_question in test_bilinear_score[prod_key]:\n",
    "            # total_no += 5\n",
    "            review_score = defaultdict()\n",
    "            for each_review in test_bilinear_score[prod_key][each_question]:\n",
    "                score = defaultdict()\n",
    "                weight_list = predicted_model[prod_key][each_question]\n",
    "                sum = weight_list[0] * test_pairscore[prod_key][each_question][each_review]['ROGUE'] + \\\n",
    "                      weight_list[1] * test_pairscore[prod_key][each_question][each_review]['BM25+'] + \\\n",
    "                      weight_list[2] * test_bilinear_score[prod_key][each_question][each_review]\n",
    "                abs_sum = abs(sum)\n",
    "                score['score'] = sum\n",
    "                score['abs_score'] = abs_sum\n",
    "                for x in review[prod_key]:\n",
    "                    if x['ID'] == each_review:\n",
    "                        score['actual_sen'] = x['actual_sen']\n",
    "                        break\n",
    "                review_score[each_review] = score\n",
    "\n",
    "\n",
    "            sorted_list = list(reversed(sorted(review_score.iteritems(), key=operator.itemgetter(1))))\n",
    "            i =0\n",
    "            #nltk.download()\n",
    "            yes, no = 0, 0\n",
    "            sid = SentimentIntensityAnalyzer()\n",
    "            for x in sorted_list:\n",
    "                sentence = x[1]['actual_sen']\n",
    "                #print sentence\n",
    "                ss = sid.polarity_scores(sentence)\n",
    "                if ss[\"compound\"] < 0:\n",
    "                    no += 1\n",
    "                else:\n",
    "                    yes += 1\n",
    "\n",
    "                i += 1\n",
    "                if i == 5:\n",
    "                    break\n",
    "            if yes >= no:\n",
    "                if test_question[prod_key][0]['A'] == True:\n",
    "                    c += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "            else:\n",
    "                if test_question[prod_key][0]['A'] == False:\n",
    "                    c += 1\n",
    "                else:\n",
    "                    i +=  1\n",
    "    #accuracy =  float(c) / float(c + i)\n",
    "    #print (\"accuracy: %s\" %accuracy)\n",
    "    return c, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class mixture_of_experts:\n",
    "    \n",
    "    def __init__(self, corpus):\n",
    "        self.rel_sim = dict()\n",
    "        self.rel_syn = dict()\n",
    "        self.voting = dict()\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def get_relevance(self):\n",
    "        r = roguel(self.corpus.review, self.corpus.question)\n",
    "        self.rel_sim = r.rogueL()\n",
    "        b = bm25(self.corpus.review, self.corpus.question, self.rel_sim)\n",
    "        self.rel_sim = b.okapi()\n",
    "        \n",
    "        s = similarity_fact(self.corpus.review, self.corpus.question)\n",
    "        cosine,tf_idf_questions,tf_idf_reviews = s.evaluate_cosine_similarity()\n",
    "\n",
    "        bilinear_score = evaluate_bilinear(cosine,tf_idf_questions,tf_idf_reviews)\n",
    "        #print(\"bilinear done\")\n",
    "        predicted_model = evaluate_model(self.rel_sim,bilinear_score,self.corpus.question)\n",
    "        \n",
    "        return predicted_model\n",
    "\n",
    "    def test_relevance(self):\n",
    "        r = roguel(self.corpus.review, self.corpus.test_question)\n",
    "        self.rel_sim = r.rogueL()\n",
    "        b = bm25(self.corpus.review, self.corpus.test_question, self.rel_sim)\n",
    "        self.rel_sim = b.okapi()\n",
    "        \n",
    "        s = similarity_fact(self.corpus.review, self.corpus.test_question)\n",
    "        cosine,tf_idf_questions,tf_idf_reviews = s.evaluate_cosine_similarity()\n",
    "        bilinear_score = evaluate_bilinear(cosine,tf_idf_questions,tf_idf_reviews)\n",
    "        return self.rel_sim, bilinear_score\n",
    "        \n",
    "    def get_voting(self, predicted_model,test_pairscore,test_bilinear_score,test_question, question, review):\n",
    "        correct, incorrect = classification(predicted_model,test_pairscore,test_bilinear_score,test_question, question, review)\n",
    "        return correct, incorrect\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we first train the parameterized weights of the model and then test it on 10% dataset as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary formation done\n",
      "training done\n",
      "testing done\n",
      "voting done\n",
      "accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "def main(question_file_name, review_file_name):\n",
    "    \n",
    "    corp = corpus(question_file_name,review_file_name)\n",
    "    corp.create_dict()\n",
    "    print \"Dictionary formation done\"\n",
    "\n",
    "    mod = mixture_of_experts(corp)\n",
    "    predicted_model = mod.get_relevance()\n",
    "    print \"training done\"\n",
    "\n",
    "    #testing\n",
    "    modtest = mixture_of_experts(corp)\n",
    "    test_pairscore,test_bilinear_score= modtest.test_relevance()\n",
    "    print \"testing done\"\n",
    "\n",
    "    c, i = mod.get_voting(predicted_model,test_pairscore,test_bilinear_score,corp.test_question, corp.question, corp.review)\n",
    "    print \"voting done\"\n",
    "    \n",
    "    accuracy =  float(c) / float(c + i)\n",
    "    print (\"accuracy: %s\" %accuracy)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    question_file_name = 'questions_Cell_Phones_and_Accessories.txt'\n",
    "    review_file_name = 'reviews_Cell_Phones_and_Accessories.txt'\n",
    "    main(question_file_name, review_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested our model for binary QA against the following datasets:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the quantitative visualization of our evaluation results against the baseline evalaution of the Moqa model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_both.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the quantitative visualization of our evaluation results against the baseline evalaution of the ro-L model, which is the learned weighted ROGUE + BM25+ model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"plot_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get list of reviews relevant to the question which can help user to find answer to their query. We use LSI and LDA model for content and topic identification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Is it long lasting ? \n",
      "\n",
      "Answers:\n",
      "\n",
      "1  this unit came with my new constructed house . it finally went out after 12 . 5 years . i hope my new one last as long . \n",
      "\n",
      "2  this model disposal was in my home when we first moved in and lasted us 9 years before it went on the blink and started to leak from the bottom . worked very well but a little noisy . replaced it with insinkerator badger 5 12 hp that has more power and is quieter hope its as reliable . \n",
      "\n",
      "3  works great . great price \n",
      "\n",
      "4  this was an exact replacement for what i already had which was in the house when i purchased it over 12 years ago . i didnt have to change any of the drain mounting hardware . hopefully this one will last as long . \n",
      "\n",
      "5  bought this disposal to replace an old badger 1 that id used for over 10 years . it came with the house so it could be much older . this one lasted less than a year and a half . i hope i just got a bad one but after looking at the other reviews i doubt it . im thinking their quality has gone downhill over the years at least with the badger line . just bought an evolution excel model hoping that they are better quality . \n",
      "\n",
      "6  product as promised shipped on time and was easy to install . works fine in the last month of operation . the badger disposer which came with my new home lasted about 7 years and then rusted through to leak . hope this one has a longer life . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from operator import itemgetter, attrgetter\n",
    "import math\n",
    "\n",
    "\n",
    "q_file = \"question_2.txt\"\n",
    "r_file = \"reviews_2.txt\"\n",
    "\n",
    "\n",
    "def create_dict(question, review):\n",
    "    make_ques(question)\n",
    "    make_review(review, question)\n",
    "\n",
    "\n",
    "def make_ques(question):\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "    with open(q_file) as fp:\n",
    "        for line in fp:\n",
    "            words = line.split()\n",
    "            #not taking open ended question for now\n",
    "            if len(words) == 0 or words[1] == \"A\":\n",
    "                continue\n",
    "            #product id\n",
    "            if words[0] not in question:\n",
    "                question[words[0]] = list()\n",
    "\n",
    "            if words[1] == \"Q\":\n",
    "                freq = list()\n",
    "                line = \"\"\n",
    "               # freq[\"words\"] = dict()\n",
    "               # freq[\"sentence\"] = list()\n",
    "               # freq[\"ID\"] = words[3]\n",
    "                for i in range(5, 5+int(words[4])):\n",
    "                    line += \" \" + words[i]\n",
    "                    if words[i] == \".\":\n",
    "                        continue\n",
    "                    freq.append(words[i])\n",
    "                # remove stop words from tokens\n",
    "                freq = [i for i in freq if not i in en_stop]\n",
    "                # stem tokens\n",
    "                freq = [p_stemmer.stem(i) for i in freq]\n",
    "                freq.append(line)\n",
    "\n",
    "                question[words[0]].append(freq)\n",
    "\n",
    "def make_review(review, question):\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    with open(r_file) as fp:\n",
    "        for line in fp:\n",
    "            words = line.split()\n",
    "            if len(words) == 0 or words[1] not in question:\n",
    "                continue\n",
    "            if words[1] not in review:\n",
    "                review[words[1]] = list()\n",
    "            freq = list()\n",
    "            line = \"\"\n",
    "            for i in range(5, 5+int(words[4])):\n",
    "                line += \" \"+words[i]\n",
    "                if words[i] == \".\":\n",
    "                    continue\n",
    "                freq.append(words[i])\n",
    "            # remove stop words from tokens\n",
    "            freq = [i for i in freq if not i in en_stop]\n",
    "            # stem tokens\n",
    "            freq = [p_stemmer.stem(i) for i in freq]\n",
    "            freq.append(line)\n",
    "\n",
    "            review[words[1]].append(freq)\n",
    "\n",
    "\n",
    "\n",
    "def find(question, review):\n",
    "    for item in question:\n",
    "        val = list()\n",
    "        r_review = list()\n",
    "        if item not in review:\n",
    "            continue\n",
    "        for x in review[item]:\n",
    "            r_review.append(x[-1])\n",
    "            val.append(x[:len(x)-1])\n",
    "        qr = corpora.Dictionary(val)\n",
    "        qr.save('auto_review.dict')\n",
    "        corpus = [qr.doc2bow(text) for text in val]\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        corpus_tfidf = tfidf[corpus]\n",
    "        lda_model = models.LsiModel(corpus_tfidf, id2word=qr, num_topics=5)\n",
    "        corpus_lsi = lda_model[corpus_tfidf]\n",
    "        #for i in range(0, len(corpus_lsi)):\n",
    "         #   print i+1, r_review[i], corpus_lsi[i]#sorted(, key=itemgetter(1), reverse=True)\n",
    "\n",
    "        c_list = dict()\n",
    "        result = sorted(corpus_lsi[-1], key=lambda x: math.fabs(x[1]), reverse=True)\n",
    "        #print result\n",
    "        for i in range(0, len(corpus_lsi) - 1):\n",
    "            for x in corpus_lsi[i]:\n",
    "                if x[0] == result[0][0]:\n",
    "                    c_list[i] = x[1]\n",
    "                    break\n",
    "\n",
    "       # for key in c_list:\n",
    "        #    print r_review[key]\n",
    "        c_list = {x: math.fabs(c_list[x]) for (x, _) in c_list.iteritems()}\n",
    "        #print c_list\n",
    "        sorted_x = sorted(c_list.items(), key=itemgetter(1), reverse=True)\n",
    "        i = 0\n",
    "        print \"Question:\", r_review[-1], \"?\", \"\\n\"\n",
    "        print \"Answers:\\n\"\n",
    "        index = 1\n",
    "        for key in sorted_x:\n",
    "            if i > 5:\n",
    "                break\n",
    "            print index, r_review[key[0]], \"\\n\"\n",
    "            i += 1\n",
    "            index += 1\n",
    "\n",
    "\n",
    "\n",
    "question, review = dict(), dict()\n",
    "create_dict(question, review)\n",
    "find(question, review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Is it long lasting ? \n",
      "\n",
      "Answers:\n",
      "\n",
      "1  before renting out my dads house we updated it including installing a badger 1 . the unit lasted 5 years but wait the house is only rented 3 months out of the year making the badgers life only 15 months . it locked up when the house was vacant and when i turned it with the provided wrench i could hear the grinding of rusty metal . to date it has yet to leak but you know it is coming when it locks up in one week of not being used . our previous ise not a badger model in our own home lasted nearly 20 years so i replaced it with another ise . if it does not last as long as the first one i will chalk it up as another example of hidden inflation cheaper made products but selling at near original prices . update manufacturers responseout of nowhere i received an email from a company rep . he had specific questions on the behavior and lifespan of the product which enabled him to diagnose the problem . after doing so he said the product should not have failed under those conditions and offered to replace the unit free of charge and free shipping . yesterday aug . 6 it arrived as promised . with all of the criticisms i have heard about american companies stories like this one need more press . as a businessman i can appreciate the pressure to produce a product at a price people are willing and able to pay while striving to do so in compliance with all the laws regulations overhead and materials costs businesses face . i never expected a company rep to read amazon reviews in order to better reach that goal . the badger 1 itself remains rated at 1 star but the company rates 5 stars . thank you ise . \n",
      "\n",
      "2  before renting out my dads house we updated it including installing a badger 1 . the unit lasted 5 years but wait the house is only rented 3 months out of the year making the badgers life only 15 months . it locked up when the house was vacant and when i turned it with the provided wrench i could hear the grinding of rusty metal . to date it has yet to leak but you know it is coming when it locks up in one week of not being used . our previous ise not a badger model in our own home lasted nearly 20 years so i replaced it with another ise . if it does not last as long as the first one i will chalk it up as another example of hidden inflation cheaper made products but selling at near original prices . update manufacturers responseout of nowhere i received an email from a company rep . he had specific questions on the behavior and lifespan of the product which enabled him to diagnose the problem . after doing so he said the product should not have failed under those conditions and offered to replace the unit free of charge and free shipping . yesterday aug . 6 it arrived as promised . with all of the criticisms i have heard about american companies stories like this one need more press . as a businessman i can appreciate the pressure to produce a product at a price people are willing and able to pay while striving to do so in compliance with all the laws regulations overhead and materials costs businesses face . i never expected a company rep to read amazon reviews in order to better reach that goal . the badger 1 itself remains rated at 1 star but the company rates 5 stars . thank you ise . \n",
      "\n",
      "3  before renting out my dads house we updated it including installing a badger 1 . the unit lasted 5 years but wait the house is only rented 3 months out of the year making the badgers life only 15 months . it locked up when the house was vacant and when i turned it with the provided wrench i could hear the grinding of rusty metal . to date it has yet to leak but you know it is coming when it locks up in one week of not being used . our previous ise not a badger model in our own home lasted nearly 20 years so i replaced it with another ise . if it does not last as long as the first one i will chalk it up as another example of hidden inflation cheaper made products but selling at near original prices . update manufacturers responseout of nowhere i received an email from a company rep . he had specific questions on the behavior and lifespan of the product which enabled him to diagnose the problem . after doing so he said the product should not have failed under those conditions and offered to replace the unit free of charge and free shipping . yesterday aug . 6 it arrived as promised . with all of the criticisms i have heard about american companies stories like this one need more press . as a businessman i can appreciate the pressure to produce a product at a price people are willing and able to pay while striving to do so in compliance with all the laws regulations overhead and materials costs businesses face . i never expected a company rep to read amazon reviews in order to better reach that goal . the badger 1 itself remains rated at 1 star but the company rates 5 stars . thank you ise . \n",
      "\n",
      "4  my wife and i remodeled our kitchen two years ago this month and installed a new insinkerator badger 1 to replace our much older insinkerator . being short on funds due to the kitchen remodel we bought the badger 1 as it basically looked the same as our old one that lasted at least 10 years . this month our badger 1 models case cracked about midcase and ran 334 right up the side to above the blades . every time the disposal ran it splattered water out the crack which we did not notice because it is inside the sink cabinet . unfortunately not enough water came out to also come out the cabinet or we would have noticed it earlier . over time the standing water warped the cabinet floor . now that i have just taken it out i noticed it is lighter than i remember the old disposal because i can hold this up with one arm while unlocking it from the sink and i remember needing my wife to loosen the locking ring while i held up the old one using two arms due to its weight . this disposers housing is plastic not metal like the old disposal the bottom half looks like dark gray painted metal but tapping on it shows it is plastic . this is just another cheap product home depot has switched over to the last 1015 years to stay financially competitive . i sure wish they would sell american quality products which lasted much much longer . \n",
      "\n",
      "5  my wife and i remodeled our kitchen two years ago this month and installed a new insinkerator badger 1 to replace our much older insinkerator . being short on funds due to the kitchen remodel we bought the badger 1 as it basically looked the same as our old one that lasted at least 10 years . this month our badger 1 models case cracked about midcase and ran 334 right up the side to above the blades . every time the disposal ran it splattered water out the crack which we did not notice because it is inside the sink cabinet . unfortunately not enough water came out to also come out the cabinet or we would have noticed it earlier . over time the standing water warped the cabinet floor . now that i have just taken it out i noticed it is lighter than i remember the old disposal because i can hold this up with one arm while unlocking it from the sink and i remember needing my wife to loosen the locking ring while i held up the old one using two arms due to its weight . this disposers housing is plastic not metal like the old disposal the bottom half looks like dark gray painted metal but tapping on it shows it is plastic . this is just another cheap product home depot has switched over to the last 1015 years to stay financially competitive . i sure wish they would sell american quality products which lasted much much longer . \n",
      "\n",
      "6  my wife and i remodeled our kitchen two years ago this month and installed a new insinkerator badger 1 to replace our much older insinkerator . being short on funds due to the kitchen remodel we bought the badger 1 as it basically looked the same as our old one that lasted at least 10 years . this month our badger 1 models case cracked about midcase and ran 334 right up the side to above the blades . every time the disposal ran it splattered water out the crack which we did not notice because it is inside the sink cabinet . unfortunately not enough water came out to also come out the cabinet or we would have noticed it earlier . over time the standing water warped the cabinet floor . now that i have just taken it out i noticed it is lighter than i remember the old disposal because i can hold this up with one arm while unlocking it from the sink and i remember needing my wife to loosen the locking ring while i held up the old one using two arms due to its weight . this disposers housing is plastic not metal like the old disposal the bottom half looks like dark gray painted metal but tapping on it shows it is plastic . this is just another cheap product home depot has switched over to the last 1015 years to stay financially competitive . i sure wish they would sell american quality products which lasted much much longer . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from operator import itemgetter, attrgetter\n",
    "import math\n",
    "\n",
    "q_file = \"question_2.txt\"\n",
    "r_file = \"reviews_2.txt\"\n",
    "\n",
    "#########################################\n",
    "# create dictionary\n",
    "########################################\n",
    "def create_dict(question, review):\n",
    "    make_ques(question)\n",
    "\n",
    "    make_review(review, question)\n",
    "\n",
    "    find(question, review)\n",
    "    \n",
    "#########################################\n",
    "# create dictionary for question\n",
    "########################################\n",
    "def make_ques(question):\n",
    "    #remove stop words and do stemming\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "    \n",
    "    #traverse each line of the file\n",
    "    with open(q_file) as fp:\n",
    "        for line in fp:\n",
    "            words = line.split()\n",
    "            \n",
    "            #ignoring answers\n",
    "            if len(words) == 0 or words[1] == \"A\":\n",
    "                continue\n",
    "                \n",
    "            #product id\n",
    "            if words[0] not in question:\n",
    "                question[words[0]] = list()\n",
    "            \n",
    "            if words[1] == \"Q\":\n",
    "                freq = list()\n",
    "                line = \"\"\n",
    "               \n",
    "                for i in range(5, 5+int(words[4])):\n",
    "                    line += \" \" + words[i]\n",
    "                    if words[i] == \".\":\n",
    "                        continue\n",
    "                    freq.append(words[i])\n",
    "                # remove stop words from tokens\n",
    "                freq = [i for i in freq if not i in en_stop]\n",
    "                # stem tokens\n",
    "                freq = [p_stemmer.stem(i) for i in freq]\n",
    "                freq.append(line)\n",
    "                    \n",
    "                question[words[0]].append(freq)\n",
    "                \n",
    "\n",
    "##########################################\n",
    "# create dictionary for review list\n",
    "###########################################\n",
    "def make_review(review, question):\n",
    "    #stop words and stemming\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    #traverse each line of the file\n",
    "    with open(r_file) as fp:\n",
    "        for line in fp:\n",
    "            words = line.split()\n",
    "            \n",
    "            #ignoring reviews not in questions\n",
    "            if len(words) == 0 or words[1] not in question:\n",
    "                continue\n",
    "                \n",
    "            if words[1] not in review:\n",
    "                review[words[1]] = list()\n",
    "            freq = list()\n",
    "            line = \"\"\n",
    "            for i in range(5, 5+int(words[4])):\n",
    "                line += \" \"+words[i]\n",
    "                if words[i] == \".\":\n",
    "                    continue\n",
    "                freq.append(words[i])\n",
    "            # remove stop words from tokens\n",
    "            freq = [i for i in freq if not i in en_stop]\n",
    "            # stem tokens\n",
    "            freq = [p_stemmer.stem(i) for i in freq]\n",
    "            freq.append(line)\n",
    "\n",
    "            review[words[1]].append(freq)\n",
    "\n",
    "\n",
    "############################################\n",
    "#find relevant review\n",
    "##########################################\n",
    "def find(question, review):\n",
    "    #for each question train model to \n",
    "    #get comparable topic\n",
    "    for item in question:\n",
    "        val = list()\n",
    "        r_review = list()\n",
    "        if item not in review:\n",
    "            continue\n",
    "        for x in review[item]:\n",
    "            r_review.append(x[-1])\n",
    "            val.append(x[:len(x)-1])\n",
    "        #create corpus with each word have id\n",
    "        qr = corpora.Dictionary(val)\n",
    "        qr.save('auto_review.dict')\n",
    "        corpus = [qr.doc2bow(text) for text in val]\n",
    "        #learn lda model for given query\n",
    "        lda_model = gensim.models.LdaModel(corpus, alpha='auto', num_topics=5, id2word=qr, passes=100)\n",
    "        corpus_lsi = lda_model[corpus]\n",
    "        \n",
    "        #sort review with maximum probable topic\n",
    "        c_list = dict()\n",
    "        result = sorted(corpus_lsi[-1], key=lambda x: math.fabs(x[1]), reverse=True)\n",
    "        c_list = dict()\n",
    "        for i in range(0, len(corpus_lsi) - 1):\n",
    "            for x in corpus_lsi[i]:\n",
    "                if x[0] == result[0][0]:\n",
    "                    c_list[i] = x[1]\n",
    "                    break\n",
    "        #create answer\n",
    "        c_list = {x: math.fabs(c_list[x]) for (x, _) in c_list.iteritems()}\n",
    "        # print c_list\n",
    "        sorted_x = sorted(c_list.items(), key=itemgetter(1), reverse=True)\n",
    "        i = 0\n",
    "        print \"Question:\", r_review[-1], \"?\", \"\\n\"\n",
    "        print \"Answers:\\n\"\n",
    "        index = 1\n",
    "        for key in sorted_x:\n",
    "            if i > 5:\n",
    "                break\n",
    "            print index, r_review[key[0]], \"\\n\"\n",
    "            i += 1\n",
    "            index += 1\n",
    "\n",
    "\n",
    "create_dict(question, review)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
